# proactive_analysis.py
"""
Syst√®me d'analyse proactive multi-dimensionnelle.
Franck creuse automatiquement les dimensions pertinentes selon le contexte.
"""

import re
import os
from typing import Dict, List, Optional, Tuple


# Patterns de colonnes synonymes pour le matching intelligent
COLUMN_PATTERNS = {
    "country": ["country", "country_code", "pays", "country_name"],
    "acquisition_type": ["acquisition_type", "acquisition_channel", "acquisition_source", "source", "canal_acquisition"],
    "acquisition_source": ["acquisition_source", "acquisition_channel", "source", "utm_source"],
    "box_name": ["box_name", "box_type", "product_name", "box", "nom_box"],
    "product_type": ["product_type", "product_category", "category", "type_produit"],
    "channel": ["channel", "canal", "sales_channel", "marketing_channel"],
    "customer_segment": ["customer_segment", "segment", "user_segment", "client_segment"],
    "boxes_received": ["boxes_received", "nb_boxes", "box_count", "nombre_box"],
    "tenure_months": ["tenure_months", "anciennete", "months_active", "mois_anciennete"],
    "order_status": ["order_status", "status", "statut", "order_state"],
    "shipment_status": ["shipment_status", "delivery_status", "statut_livraison"],
    "subscription_type": ["subscription_type", "sub_type", "type_abonnement"],
    "is_active": ["is_active", "active", "actif", "status"],
    "payment_method": ["payment_method", "paiement", "payment_type"],
    "tenure_bucket": ["tenure_bucket", "anciennete_bucket", "tenure_group"],
    "last_box_name": ["last_box_name", "derniere_box", "last_box"]
}


# Mapping contexte ‚Üí dimensions pertinentes √† explorer
CONTEXT_DIMENSIONS = {
    "churn": {
        "dimensions": [
            ("acquisition_type", "Type d'acquisition"),
            ("boxes_received", "Nombre de box re√ßues"),
            ("tenure_months", "Anciennet√© (mois)"),
            ("last_box_name", "Derni√®re box re√ßue"),
            ("customer_segment", "Segment client"),
            ("country", "Pays")
        ],
        "keywords": ["churn", "d√©sabonnement", "d√©sinscrit", "churned", "attrition", "r√©sili√©"]
    },
    "revenue": {
        "dimensions": [
            ("country", "Pays"),
            ("product_category", "Cat√©gorie produit"),
            ("channel", "Canal"),
            ("customer_segment", "Segment client"),
            ("box_name", "Nom de la box"),
            ("payment_method", "Moyen de paiement")
        ],
        "keywords": ["ca", "chiffre", "revenue", "revenu", "total_amount", "gmv", "‚Ç¨", "montant"]
    },
    "orders": {
        "dimensions": [
            ("country", "Pays"),
            ("product_type", "Type de produit"),
            ("acquisition_source", "Source d'acquisition"),
            ("box_name", "Nom de la box"),
            ("order_status", "Statut commande"),
            ("channel", "Canal")
        ],
        "keywords": ["commande", "order", "achat", "purchase", "vente", "sale", "transaction"]
    },
    "subscriptions": {
        "dimensions": [
            ("country", "Pays"),
            ("subscription_type", "Type abonnement"),
            ("acquisition_type", "Type d'acquisition"),
            ("tenure_bucket", "Anciennet√©"),
            ("is_active", "Statut"),
            ("box_name", "Box souscrite")
        ],
        "keywords": ["abonnement", "subscription", "sub", "souscription", "abonn√©"]
    },
    "boxes": {
        "dimensions": [
            ("box_name", "Nom de la box"),
            ("country", "Pays"),
            ("customer_segment", "Segment"),
            ("acquisition_type", "Type acquisition"),
            ("shipment_status", "Statut livraison")
        ],
        "keywords": ["box", "colis", "envoi", "shipment", "livraison"]
    },
    "users": {
        "dimensions": [
            ("country", "Pays"),
            ("acquisition_type", "Type d'acquisition"),
            ("customer_segment", "Segment"),
            ("is_active", "Statut actif"),
            ("tenure_bucket", "Anciennet√©")
        ],
        "keywords": ["user", "utilisateur", "client", "customer", "membre"]
    }
}


def detect_analysis_context(user_prompt: str, sql_query: str) -> Optional[Dict]:
    """
    D√©tecte le type d'analyse bas√© sur le prompt utilisateur et la requ√™te SQL.
    Retourne le contexte avec les dimensions pertinentes √† explorer.
    """
    prompt_lower = user_prompt.lower() if user_prompt else ""
    query_lower = sql_query.lower()

    # Scorer chaque contexte
    context_scores = {}

    for context_type, context_info in CONTEXT_DIMENSIONS.items():
        score = 0

        # Compter les keywords trouv√©s
        for keyword in context_info["keywords"]:
            if keyword in prompt_lower:
                score += 3  # Poids fort pour le prompt
            if keyword in query_lower:
                score += 1  # Poids faible pour la requ√™te

        if score > 0:
            context_scores[context_type] = score

    # Retourner le contexte avec le meilleur score
    if context_scores:
        best_context = max(context_scores, key=context_scores.get)
        return {
            "type": best_context,
            "dimensions": CONTEXT_DIMENSIONS[best_context]["dimensions"],
            "score": context_scores[best_context]
        }

    return None


def extract_available_columns(sql_query: str) -> List[str]:
    """
    Extrait les colonnes disponibles dans la requ√™te (FROM, WHERE, etc.).
    Cela aide √† v√©rifier quelles dimensions sont r√©ellement utilisables.
    """
    # Pour simplifier, on va juste extraire les noms de colonnes mentionn√©s
    # Pattern: WHERE column = ... ou column BETWEEN ... ou SELECT column
    pattern = r'\b([a-zA-Z_][a-zA-Z0-9_]*)\s*(?:=|>=|<=|<>|BETWEEN|IN|LIKE)'
    matches = re.findall(pattern, sql_query, re.IGNORECASE)
    return list(set(matches))


def extract_table_from_query(sql_query: str) -> Optional[str]:
    """
    Extrait la table principale d'une requ√™te SQL.
    Retourne au format 'project.dataset.table' ou 'dataset.table'.
    """
    # Pattern pour capturer : FROM `project.dataset.table` ou FROM dataset.table
    patterns = [
        r'FROM\s+`([^`]+)`',  # FROM `project.dataset.table`
        r'FROM\s+([a-zA-Z0-9_-]+\.[a-zA-Z0-9_-]+\.[a-zA-Z0-9_-]+)',  # FROM project.dataset.table
        r'FROM\s+([a-zA-Z0-9_-]+\.[a-zA-Z0-9_-]+)',  # FROM dataset.table
    ]

    for pattern in patterns:
        match = re.search(pattern, sql_query, re.IGNORECASE)
        if match:
            return match.group(1)

    return None


def extract_main_table_alias(sql_query: str) -> Optional[str]:
    """
    Extrait l'alias de la table principale si la requ√™te contient des JOINs.
    Retourne l'alias ou None.

    Exemples:
    - "FROM sales.box_sales AS t1" ‚Üí "t1"
    - "FROM sales.box_sales t1" ‚Üí "t1"
    - "FROM sales.box_sales" ‚Üí None
    """
    # Pattern pour capturer : FROM table AS alias ou FROM table alias
    patterns = [
        r'FROM\s+`[^`]+`\s+(?:AS\s+)?([a-zA-Z0-9_]+)',  # FROM `table` AS alias ou FROM `table` alias
        r'FROM\s+[a-zA-Z0-9_.-]+\s+(?:AS\s+)?([a-zA-Z0-9_]+)',  # FROM table AS alias ou FROM table alias
    ]

    for pattern in patterns:
        match = re.search(pattern, sql_query, re.IGNORECASE)
        if match:
            alias = match.group(1)
            # V√©rifier que c'est bien un alias et pas un mot-cl√© SQL
            sql_keywords = ['WHERE', 'JOIN', 'LEFT', 'RIGHT', 'INNER', 'OUTER', 'ON', 'GROUP', 'ORDER', 'LIMIT', 'HAVING']
            if alias.upper() not in sql_keywords:
                return alias

    return None


def has_joins(sql_query: str) -> bool:
    """D√©tecte si la requ√™te contient des JOINs."""
    return bool(re.search(r'\b(JOIN|LEFT JOIN|RIGHT JOIN|INNER JOIN|OUTER JOIN)\b', sql_query, re.IGNORECASE))


def get_table_columns(client, table_ref: str) -> List[Tuple[str, str]]:
    """
    R√©cup√®re les colonnes disponibles d'une table via INFORMATION_SCHEMA.
    Retourne une liste de (column_name, data_type).
    """
    try:
        # Parser table_ref
        parts = table_ref.split('.')
        if len(parts) == 3:
            project_id, dataset_id, table_id = parts
        elif len(parts) == 2:
            # Utiliser le projet par d√©faut du client
            project_id = client.project
            dataset_id, table_id = parts
        else:
            return []

        # Requ√™te INFORMATION_SCHEMA avec types
        query = f"""
        SELECT column_name, data_type
        FROM `{project_id}.{dataset_id}.INFORMATION_SCHEMA.COLUMNS`
        WHERE table_name = '{table_id}'
        ORDER BY ordinal_position
        """

        job = client.query(query)
        results = list(job.result(timeout=10))

        # Retourner (nom, type)
        return [(row.column_name, row.data_type) for row in results]

    except Exception as e:
        print(f"[Proactive] Erreur r√©cup√©ration colonnes pour {table_ref}: {e}")
        return []


def is_likely_dimension_column(column_name: str, data_type: str) -> bool:
    """
    D√©termine si une colonne est probablement une dimension pertinente.
    Exclut les IDs, cl√©s, dates, m√©triques num√©riques.
    """
    col_lower = column_name.lower()

    # Types accept√©s pour les dimensions
    dimension_types = ["STRING", "INT64", "INTEGER", "BOOL", "BOOLEAN"]
    if data_type not in dimension_types:
        return False

    # Exclusions : IDs, cl√©s, dates, timestamps
    exclusions = [
        "_id", "_key", "id_", "key_",
        "_date", "_time", "date_", "time_",
        "_at", "_timestamp", "created_", "updated_",
        "_uuid", "_hash", "_token"
    ]

    for exclusion in exclusions:
        if exclusion in col_lower:
            return False

    return True


def auto_discover_dimensions(columns_with_types: List[Tuple[str, str]], max_dimensions: int = 10) -> List[str]:
    """
    D√©couvre automatiquement les dimensions pertinentes parmi toutes les colonnes.
    Retourne les colonnes qui sont probablement des dimensions int√©ressantes.

    Args:
        columns_with_types: Liste de (column_name, data_type)
        max_dimensions: Nombre max de dimensions √† retourner

    Returns:
        Liste de noms de colonnes pertinentes
    """
    # Mots-cl√©s qui indiquent une dimension pertinente (boost de priorit√©)
    priority_keywords = [
        "country", "pays",
        "type", "category", "categorie",
        "channel", "canal", "source",
        "status", "statut", "state",
        "segment", "group", "groupe",
        "name", "nom",
        "box", "product", "produit",
        "acquisition", "origin", "origine"
    ]

    candidates = []

    for col_name, data_type in columns_with_types:
        # Filtrer d'abord par type et exclusions
        if not is_likely_dimension_column(col_name, data_type):
            continue

        # Calculer un score de pertinence
        col_lower = col_name.lower()
        score = 0

        # Boost si contient un mot-cl√© prioritaire
        for keyword in priority_keywords:
            if keyword in col_lower:
                score += 10

        # P√©nalit√© pour colonnes avec beaucoup d'underscores (souvent des colonnes techniques)
        underscore_count = col_name.count('_')
        if underscore_count > 4:
            score -= 5

        # Boost pour colonnes courtes (souvent plus simples et pertinentes)
        if len(col_name) < 20:
            score += 2

        candidates.append((col_name, score))

    # Trier par score d√©croissant
    candidates.sort(key=lambda x: x[1], reverse=True)

    # Retourner les top N
    return [col_name for col_name, score in candidates[:max_dimensions]]


def match_dimension_to_column(dimension: str, available_columns: List[str]) -> Optional[str]:
    """
    Trouve la colonne r√©elle qui correspond √† une dimension souhait√©e.
    Utilise un matching fuzzy beaucoup plus permissif.

    Args:
        dimension: Nom de dimension souhait√© (ex: "country")
        available_columns: Liste des colonnes disponibles dans la table

    Returns:
        Le nom de la colonne r√©elle, ou None si pas de match
    """
    # Convertir tout en lowercase pour le matching
    available_lower = [col.lower() for col in available_columns]
    dimension_lower = dimension.lower()

    # 1. Match exact
    if dimension_lower in available_lower:
        idx = available_lower.index(dimension_lower)
        return available_columns[idx]

    # 2. Match via patterns synonymes
    if dimension_lower in COLUMN_PATTERNS:
        for pattern in COLUMN_PATTERNS[dimension_lower]:
            pattern_lower = pattern.lower()
            # Match exact du pattern
            if pattern_lower in available_lower:
                idx = available_lower.index(pattern_lower)
                return available_columns[idx]

            # Match partiel : pattern contenu dans colonne (avec pr√©fixes dw_, dim_, etc.)
            for i, col_lower in enumerate(available_lower):
                if pattern_lower in col_lower:
                    return available_columns[i]

    # 3. Match fuzzy : extraire les mots-cl√©s de la dimension
    # Ex: "acquisition_source" ‚Üí ["acquisition", "source"]
    dimension_words = set(re.split(r'[_\s-]', dimension_lower))
    dimension_words = {w for w in dimension_words if len(w) > 2}  # Mots > 2 caract√®res

    if dimension_words:
        best_match = None
        best_score = 0

        for i, col in enumerate(available_columns):
            col_lower = col.lower()
            col_words = set(re.split(r'[_\s-]', col_lower))

            # Compter les mots en commun
            common_words = dimension_words & col_words
            score = len(common_words)

            # Boost si dimension_word est contenu dans un col_word
            for dim_word in dimension_words:
                for col_word in col_words:
                    if dim_word in col_word or col_word in dim_word:
                        score += 0.5

            if score > best_score:
                best_score = score
                best_match = col

        # Accepter le match si score > 0
        if best_score > 0:
            return best_match

    return None


def get_validated_dimensions(
    client,
    sql_query: str,
    desired_dimensions: List[Tuple[str, str]],
    use_auto_discovery: bool = True
) -> List[Tuple[str, str]]:
    """
    Valide les dimensions souhait√©es contre les colonnes r√©elles de la table.
    Peut aussi d√©couvrir automatiquement les dimensions pertinentes.

    Args:
        client: Client BigQuery
        sql_query: Requ√™te SQL originale
        desired_dimensions: Liste de (dimension_name, label) souhait√©e
        use_auto_discovery: Si True, d√©couvre automatiquement des dimensions suppl√©mentaires

    Returns:
        Liste de (real_column_name, label) pour les dimensions valid√©es
    """
    # Extraire la table de la requ√™te
    table_ref = extract_table_from_query(sql_query)
    if not table_ref:
        print("[Proactive] Impossible d'extraire la table de la requ√™te")
        return []

    print(f"[Proactive] Table d√©tect√©e : {table_ref}")

    # R√©cup√©rer les colonnes disponibles avec leurs types
    columns_with_types = get_table_columns(client, table_ref)
    if not columns_with_types:
        print("[Proactive] Aucune colonne r√©cup√©r√©e via INFORMATION_SCHEMA")
        return []

    print(f"[Proactive] Colonnes disponibles : {len(columns_with_types)}")

    # Extraire juste les noms pour le matching
    available_column_names = [col_name for col_name, _ in columns_with_types]

    validated = []

    # 1. Essayer de matcher les dimensions souhait√©es
    for dimension, label in desired_dimensions:
        real_column = match_dimension_to_column(dimension, available_column_names)
        if real_column:
            validated.append((real_column, label))
            print(f"[Proactive] ‚úì Match : {dimension} ‚Üí {real_column}")
        else:
            print(f"[Proactive] ‚úó Pas de match pour : {dimension}")

    # 2. Auto-discovery : si pas assez de matches, d√©couvrir des dimensions automatiquement
    if use_auto_discovery and len(validated) < 3:
        print("[Proactive] Auto-discovery : recherche de dimensions suppl√©mentaires...")

        # D√©couvrir les dimensions pertinentes
        discovered = auto_discover_dimensions(columns_with_types, max_dimensions=5)

        # Ajouter celles qui ne sont pas d√©j√† dans validated
        validated_names = {col_name for col_name, _ in validated}

        for col_name in discovered:
            if col_name not in validated_names:
                # G√©n√©rer un label lisible √† partir du nom de colonne
                label = col_name.replace('_', ' ').replace('dw ', '').title()
                validated.append((col_name, label))
                print(f"[Proactive] üîç Auto-d√©couverte : {col_name} ({label})")

                # Limiter √† 3 dimensions au total
                if len(validated) >= 3:
                    break

    return validated


def generate_drill_down_query(original_query: str, dimension: str) -> Optional[str]:
    """
    G√©n√®re une requ√™te de drill-down en ajoutant un GROUP BY sur la dimension.
    Garde la m√™me logique WHERE mais ajoute la dimension dans le SELECT et GROUP BY.

    G√®re les JOINs en pr√©fixant les colonnes avec l'alias de table si n√©cessaire.
    """
    try:
        # üÜï D√©tecter si la requ√™te contient des JOINs (colonnes potentiellement ambigu√´s)
        dimension_to_use = dimension
        if has_joins(original_query):
            alias = extract_main_table_alias(original_query)
            if alias:
                # Pr√©fixer la dimension avec l'alias pour √©viter l'ambigu√Øt√©
                dimension_to_use = f"{alias}.{dimension}"
                print(f"[Proactive] JOIN d√©tect√© ‚Üí pr√©fixe : {dimension} ‚Üí {dimension_to_use}")
            else:
                print(f"[Proactive] JOIN d√©tect√© mais pas d'alias trouv√© ‚Üí risque d'ambigu√Øt√©")

        query_upper = original_query.upper()

        # V√©rifier si la requ√™te a d√©j√† un GROUP BY
        has_group_by = "GROUP BY" in query_upper

        if has_group_by:
            # Ajouter la dimension au GROUP BY existant
            pattern = r'(GROUP\s+BY\s+)([^ORDER|HAVING|LIMIT]+)'
            match = re.search(pattern, original_query, re.IGNORECASE)

            if match:
                group_by_prefix = match.group(1)
                existing_cols = match.group(2).strip()

                # V√©rifier si la dimension n'est pas d√©j√† dans le GROUP BY
                if dimension.lower() not in existing_cols.lower():
                    new_group_by = f"{group_by_prefix}{dimension_to_use}, {existing_cols}"
                    new_query = re.sub(
                        pattern,
                        new_group_by,
                        original_query,
                        count=1,
                        flags=re.IGNORECASE
                    )

                    # Ajouter la dimension dans le SELECT aussi
                    new_query = re.sub(
                        r'(SELECT\s+)',
                        f'\\1{dimension_to_use}, ',
                        new_query,
                        count=1,
                        flags=re.IGNORECASE
                    )

                    return new_query
        else:
            # Pas de GROUP BY : en cr√©er un
            # 1. Ajouter dimension dans SELECT
            new_query = re.sub(
                r'(SELECT\s+)',
                f'\\1{dimension_to_use}, ',
                original_query,
                count=1,
                flags=re.IGNORECASE
            )

            # 2. Supprimer LIMIT existant (sera ajout√© par _enforce_limit)
            new_query = re.sub(r'\s*LIMIT\s+\d+\s*$', '', new_query, flags=re.IGNORECASE)

            # 3. Ajouter ORDER BY si pr√©sent, sinon ajouter GROUP BY avant
            if "ORDER BY" in new_query.upper():
                # Ins√©rer GROUP BY avant ORDER BY
                new_query = re.sub(
                    r'(\s+ORDER\s+BY)',
                    f'\nGROUP BY {dimension_to_use}\\1',
                    new_query,
                    count=1,
                    flags=re.IGNORECASE
                )
            else:
                # Ajouter GROUP BY √† la fin
                new_query = f"{new_query.rstrip()}\nGROUP BY {dimension_to_use}"

            return new_query

    except Exception as e:
        print(f"[Proactive] Erreur g√©n√©ration requ√™te pour {dimension}: {e}")
        return None

    return None


def execute_drill_downs(
    client,
    original_query: str,
    dimensions: List[Tuple[str, str]],
    thread_ts: str,
    timeout: int
) -> Dict:
    """
    Ex√©cute les requ√™tes de drill-down pour chaque dimension pertinente.
    Retourne un dict {dimension: {label: str, results: list}}
    """
    from bigquery_tools import _enforce_limit

    results = {}
    max_drill_downs = int(os.getenv("MAX_DRILL_DOWNS", "3"))

    # üÜï VALIDATION : V√©rifier que les dimensions existent vraiment dans la table
    print(f"[Proactive] Validation des {len(dimensions)} dimensions souhait√©es...")
    validated_dimensions = get_validated_dimensions(client, original_query, dimensions)

    if not validated_dimensions:
        print("[Proactive] Aucune dimension valid√©e ‚Äî skip drill-downs")
        return {}

    print(f"[Proactive] {len(validated_dimensions)} dimension(s) valid√©e(s) sur {len(dimensions)}")

    for dimension, label in validated_dimensions[:max_drill_downs]:
        try:
            drill_query = generate_drill_down_query(original_query, dimension)

            if not drill_query:
                print(f"[Proactive] Impossible de g√©n√©rer requ√™te pour {dimension}")
                continue

            # Ex√©cuter la requ√™te
            enforced_query = _enforce_limit(drill_query)

            print(f"[Proactive] Ex√©cution drill-down sur {dimension}...")
            job = client.query(enforced_query)
            rows = list(job.result(timeout=timeout))

            if rows and len(rows) > 0:
                # Convertir en liste de dicts (max 10 lignes par dimension)
                rows_data = [dict(row) for row in rows[:10]]

                results[dimension] = {
                    "label": label,
                    "results": rows_data
                }

                print(f"[Proactive] ‚úì Drill-down {dimension}: {len(rows_data)} r√©sultats")
            else:
                print(f"[Proactive] ‚úó Drill-down {dimension}: aucun r√©sultat")

        except Exception as e:
            print(f"[Proactive] ‚úó Erreur drill-down {dimension}: {str(e)[:100]}")
            continue

    return results


def format_proactive_analysis(main_result: list, drill_down_results: Dict, context_type: str) -> str:
    """
    Formate les r√©sultats de l'analyse proactive multi-dimensionnelle.
    Pr√©sente les drill-downs de mani√®re claire et actionnable.
    """
    if not drill_down_results:
        return None

    output_lines = []
    output_lines.append("\n\n" + "="*60)
    output_lines.append("üîç **ANALYSE PROACTIVE MULTI-DIMENSIONNELLE**")
    output_lines.append(f"_Franck a automatiquement explor√© {len(drill_down_results)} dimensions pertinentes pour le contexte '{context_type}' :_\n")

    for dimension, data in drill_down_results.items():
        label = data["label"]
        results = data["results"]

        if not results:
            continue

        output_lines.append(f"### üìä Breakdown par **{label}**")

        # D√©tecter les colonnes de m√©trique (num√©riques)
        first_row = results[0]
        metric_cols = [k for k, v in first_row.items()
                      if isinstance(v, (int, float)) and k != dimension]

        if not metric_cols:
            output_lines.append("  _(Aucune m√©trique num√©rique trouv√©e)_\n")
            continue

        # Trier les r√©sultats par la premi√®re m√©trique (desc)
        results_sorted = sorted(
            results,
            key=lambda x: x.get(metric_cols[0], 0) if x.get(metric_cols[0]) is not None else 0,
            reverse=True
        )

        # Calculer le total pour les pourcentages
        total_value = sum(row.get(metric_cols[0], 0) or 0 for row in results_sorted)

        # Formater chaque ligne (Top 5)
        for i, row in enumerate(results_sorted[:5], 1):
            dim_value = row.get(dimension, "N/A")

            # Formater les m√©triques
            metrics_parts = []
            for col in metric_cols:
                value = row.get(col, 0) or 0

                if isinstance(value, float):
                    metrics_parts.append(f"{col}={value:,.2f}")
                else:
                    metrics_parts.append(f"{col}={value:,}")

                # Ajouter le pourcentage du total pour la premi√®re m√©trique
                if col == metric_cols[0] and total_value > 0:
                    pct = (value / total_value * 100) if value else 0
                    metrics_parts.append(f"({pct:.1f}%)")
                    break  # On ne montre le % que pour la premi√®re m√©trique

            metrics_formatted = " | ".join(metrics_parts)

            # Emoji pour le top 3
            emoji = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else "  "

            output_lines.append(f"  {emoji} **{dim_value}** : {metrics_formatted}")

        if len(results_sorted) > 5:
            output_lines.append(f"  _... et {len(results_sorted) - 5} autres valeurs_")

        output_lines.append("")

    output_lines.append("="*60)

    return "\n".join(output_lines)
