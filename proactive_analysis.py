# proactive_analysis.py
"""
SystÃ¨me d'analyse proactive multi-dimensionnelle.
Franck creuse automatiquement les dimensions pertinentes selon le contexte.
"""

import re
import os
from typing import Dict, List, Optional, Tuple


# Patterns de colonnes synonymes pour le matching intelligent
COLUMN_PATTERNS = {
    "country": ["country", "country_code", "pays", "country_name"],
    "acquisition_type": ["acquisition_type", "acquisition_channel", "acquisition_source", "source", "canal_acquisition"],
    "acquisition_source": ["acquisition_source", "acquisition_channel", "source", "utm_source"],
    "box_name": ["box_name", "box_type", "product_name", "box", "nom_box"],
    "product_type": ["product_type", "product_category", "category", "type_produit"],
    "channel": ["channel", "canal", "sales_channel", "marketing_channel"],
    "customer_segment": ["customer_segment", "segment", "user_segment", "client_segment"],
    "boxes_received": ["boxes_received", "nb_boxes", "box_count", "nombre_box"],
    "tenure_months": ["tenure_months", "anciennete", "months_active", "mois_anciennete"],
    "order_status": ["order_status", "status", "statut", "order_state"],
    "shipment_status": ["shipment_status", "delivery_status", "statut_livraison"],
    "subscription_type": ["subscription_type", "sub_type", "type_abonnement"],
    "is_active": ["is_active", "active", "actif", "status"],
    "payment_method": ["payment_method", "paiement", "payment_type"],
    "tenure_bucket": ["tenure_bucket", "anciennete_bucket", "tenure_group"],
    "last_box_name": ["last_box_name", "derniere_box", "last_box"]
}


# Mapping contexte â†’ dimensions pertinentes Ã  explorer
CONTEXT_DIMENSIONS = {
    "churn": {
        "dimensions": [
            ("acquisition_type", "Type d'acquisition"),
            ("boxes_received", "Nombre de box reÃ§ues"),
            ("tenure_months", "AnciennetÃ© (mois)"),
            ("last_box_name", "DerniÃ¨re box reÃ§ue"),
            ("customer_segment", "Segment client"),
            ("country", "Pays")
        ],
        "keywords": ["churn", "dÃ©sabonnement", "dÃ©sinscrit", "churned", "attrition", "rÃ©siliÃ©"]
    },
    "revenue": {
        "dimensions": [
            ("country", "Pays"),
            ("product_category", "CatÃ©gorie produit"),
            ("channel", "Canal"),
            ("customer_segment", "Segment client"),
            ("box_name", "Nom de la box"),
            ("payment_method", "Moyen de paiement")
        ],
        "keywords": ["ca", "chiffre", "revenue", "revenu", "total_amount", "gmv", "â‚¬", "montant"]
    },
    "orders": {
        "dimensions": [
            ("country", "Pays"),
            ("product_type", "Type de produit"),
            ("acquisition_source", "Source d'acquisition"),
            ("box_name", "Nom de la box"),
            ("order_status", "Statut commande"),
            ("channel", "Canal")
        ],
        "keywords": ["commande", "order", "achat", "purchase", "vente", "sale", "transaction"]
    },
    "subscriptions": {
        "dimensions": [
            ("country", "Pays"),
            ("subscription_type", "Type abonnement"),
            ("acquisition_type", "Type d'acquisition"),
            ("tenure_bucket", "AnciennetÃ©"),
            ("is_active", "Statut"),
            ("box_name", "Box souscrite")
        ],
        "keywords": ["abonnement", "subscription", "sub", "souscription", "abonnÃ©"]
    },
    "boxes": {
        "dimensions": [
            ("box_name", "Nom de la box"),
            ("country", "Pays"),
            ("customer_segment", "Segment"),
            ("acquisition_type", "Type acquisition"),
            ("shipment_status", "Statut livraison")
        ],
        "keywords": ["box", "colis", "envoi", "shipment", "livraison"]
    },
    "users": {
        "dimensions": [
            ("country", "Pays"),
            ("acquisition_type", "Type d'acquisition"),
            ("customer_segment", "Segment"),
            ("is_active", "Statut actif"),
            ("tenure_bucket", "AnciennetÃ©")
        ],
        "keywords": ["user", "utilisateur", "client", "customer", "membre"]
    }
}


def detect_analysis_context(user_prompt: str, sql_query: str) -> Optional[Dict]:
    """
    DÃ©tecte le type d'analyse basÃ© sur le prompt utilisateur et la requÃªte SQL.
    Retourne le contexte avec les dimensions pertinentes Ã  explorer.
    """
    prompt_lower = user_prompt.lower() if user_prompt else ""
    query_lower = sql_query.lower()

    # Scorer chaque contexte
    context_scores = {}

    for context_type, context_info in CONTEXT_DIMENSIONS.items():
        score = 0

        # Compter les keywords trouvÃ©s
        for keyword in context_info["keywords"]:
            if keyword in prompt_lower:
                score += 3  # Poids fort pour le prompt
            if keyword in query_lower:
                score += 1  # Poids faible pour la requÃªte

        if score > 0:
            context_scores[context_type] = score

    # Retourner le contexte avec le meilleur score
    if context_scores:
        best_context = max(context_scores, key=context_scores.get)
        return {
            "type": best_context,
            "dimensions": CONTEXT_DIMENSIONS[best_context]["dimensions"],
            "score": context_scores[best_context]
        }

    return None


def extract_available_columns(sql_query: str) -> List[str]:
    """
    Extrait les colonnes disponibles dans la requÃªte (FROM, WHERE, etc.).
    Cela aide Ã  vÃ©rifier quelles dimensions sont rÃ©ellement utilisables.
    """
    # Pour simplifier, on va juste extraire les noms de colonnes mentionnÃ©s
    # Pattern: WHERE column = ... ou column BETWEEN ... ou SELECT column
    pattern = r'\b([a-zA-Z_][a-zA-Z0-9_]*)\s*(?:=|>=|<=|<>|BETWEEN|IN|LIKE)'
    matches = re.findall(pattern, sql_query, re.IGNORECASE)
    return list(set(matches))


def extract_table_from_query(sql_query: str) -> Optional[str]:
    """
    Extrait la table principale d'une requÃªte SQL.
    Retourne au format 'project.dataset.table' ou 'dataset.table'.
    """
    # Pattern pour capturer : FROM `project.dataset.table` ou FROM dataset.table
    patterns = [
        r'FROM\s+`([^`]+)`',  # FROM `project.dataset.table`
        r'FROM\s+([a-zA-Z0-9_-]+\.[a-zA-Z0-9_-]+\.[a-zA-Z0-9_-]+)',  # FROM project.dataset.table
        r'FROM\s+([a-zA-Z0-9_-]+\.[a-zA-Z0-9_-]+)',  # FROM dataset.table
    ]

    for pattern in patterns:
        match = re.search(pattern, sql_query, re.IGNORECASE)
        if match:
            return match.group(1)

    return None


def get_table_columns(client, table_ref: str) -> List[str]:
    """
    RÃ©cupÃ¨re les colonnes disponibles d'une table via INFORMATION_SCHEMA.
    Retourne une liste de noms de colonnes en lowercase.
    """
    try:
        # Parser table_ref
        parts = table_ref.split('.')
        if len(parts) == 3:
            project_id, dataset_id, table_id = parts
        elif len(parts) == 2:
            # Utiliser le projet par dÃ©faut du client
            project_id = client.project
            dataset_id, table_id = parts
        else:
            return []

        # RequÃªte INFORMATION_SCHEMA
        query = f"""
        SELECT column_name
        FROM `{project_id}.{dataset_id}.INFORMATION_SCHEMA.COLUMNS`
        WHERE table_name = '{table_id}'
        """

        job = client.query(query)
        results = list(job.result(timeout=10))

        # Retourner les noms de colonnes en lowercase
        return [row.column_name.lower() for row in results]

    except Exception as e:
        print(f"[Proactive] Erreur rÃ©cupÃ©ration colonnes pour {table_ref}: {e}")
        return []


def match_dimension_to_column(dimension: str, available_columns: List[str]) -> Optional[str]:
    """
    Trouve la colonne rÃ©elle qui correspond Ã  une dimension souhaitÃ©e.
    Utilise des patterns de matching intelligent.

    Args:
        dimension: Nom de dimension souhaitÃ© (ex: "country")
        available_columns: Liste des colonnes disponibles dans la table

    Returns:
        Le nom de la colonne rÃ©elle, ou None si pas de match
    """
    # Convertir tout en lowercase pour le matching
    available_lower = [col.lower() for col in available_columns]
    dimension_lower = dimension.lower()

    # 1. Match exact
    if dimension_lower in available_lower:
        idx = available_lower.index(dimension_lower)
        return available_columns[idx]

    # 2. Match via patterns synonymes
    if dimension_lower in COLUMN_PATTERNS:
        for pattern in COLUMN_PATTERNS[dimension_lower]:
            if pattern.lower() in available_lower:
                idx = available_lower.index(pattern.lower())
                return available_columns[idx]

    # 3. Match partiel (contient le mot)
    for col in available_columns:
        col_lower = col.lower()
        # Si la dimension est contenue dans le nom de colonne
        if dimension_lower in col_lower or col_lower in dimension_lower:
            return col

    return None


def get_validated_dimensions(
    client,
    sql_query: str,
    desired_dimensions: List[Tuple[str, str]]
) -> List[Tuple[str, str]]:
    """
    Valide les dimensions souhaitÃ©es contre les colonnes rÃ©elles de la table.
    Retourne uniquement les dimensions qui existent vraiment.

    Args:
        client: Client BigQuery
        sql_query: RequÃªte SQL originale
        desired_dimensions: Liste de (dimension_name, label)

    Returns:
        Liste de (real_column_name, label) pour les dimensions validÃ©es
    """
    # Extraire la table de la requÃªte
    table_ref = extract_table_from_query(sql_query)
    if not table_ref:
        print("[Proactive] Impossible d'extraire la table de la requÃªte")
        return []

    print(f"[Proactive] Table dÃ©tectÃ©e : {table_ref}")

    # RÃ©cupÃ©rer les colonnes disponibles
    available_columns = get_table_columns(client, table_ref)
    if not available_columns:
        print("[Proactive] Aucune colonne rÃ©cupÃ©rÃ©e via INFORMATION_SCHEMA")
        return []

    print(f"[Proactive] Colonnes disponibles : {len(available_columns)}")

    # Matcher chaque dimension souhaitÃ©e avec une colonne rÃ©elle
    validated = []
    for dimension, label in desired_dimensions:
        real_column = match_dimension_to_column(dimension, available_columns)
        if real_column:
            validated.append((real_column, label))
            print(f"[Proactive] âœ“ Match : {dimension} â†’ {real_column}")
        else:
            print(f"[Proactive] âœ— Pas de match pour : {dimension}")

    return validated


def generate_drill_down_query(original_query: str, dimension: str) -> Optional[str]:
    """
    GÃ©nÃ¨re une requÃªte de drill-down en ajoutant un GROUP BY sur la dimension.
    Garde la mÃªme logique WHERE mais ajoute la dimension dans le SELECT et GROUP BY.
    """
    try:
        query_upper = original_query.upper()

        # VÃ©rifier si la requÃªte a dÃ©jÃ  un GROUP BY
        has_group_by = "GROUP BY" in query_upper

        if has_group_by:
            # Ajouter la dimension au GROUP BY existant
            pattern = r'(GROUP\s+BY\s+)([^ORDER|HAVING|LIMIT]+)'
            match = re.search(pattern, original_query, re.IGNORECASE)

            if match:
                group_by_prefix = match.group(1)
                existing_cols = match.group(2).strip()

                # VÃ©rifier si la dimension n'est pas dÃ©jÃ  dans le GROUP BY
                if dimension.lower() not in existing_cols.lower():
                    new_group_by = f"{group_by_prefix}{dimension}, {existing_cols}"
                    new_query = re.sub(
                        pattern,
                        new_group_by,
                        original_query,
                        count=1,
                        flags=re.IGNORECASE
                    )

                    # Ajouter la dimension dans le SELECT aussi
                    new_query = re.sub(
                        r'(SELECT\s+)',
                        f'\\1{dimension}, ',
                        new_query,
                        count=1,
                        flags=re.IGNORECASE
                    )

                    return new_query
        else:
            # Pas de GROUP BY : en crÃ©er un
            # 1. Ajouter dimension dans SELECT
            new_query = re.sub(
                r'(SELECT\s+)',
                f'\\1{dimension}, ',
                original_query,
                count=1,
                flags=re.IGNORECASE
            )

            # 2. Supprimer LIMIT existant (sera ajoutÃ© par _enforce_limit)
            new_query = re.sub(r'\s*LIMIT\s+\d+\s*$', '', new_query, flags=re.IGNORECASE)

            # 3. Ajouter ORDER BY si prÃ©sent, sinon ajouter GROUP BY avant
            if "ORDER BY" in new_query.upper():
                # InsÃ©rer GROUP BY avant ORDER BY
                new_query = re.sub(
                    r'(\s+ORDER\s+BY)',
                    f'\nGROUP BY {dimension}\\1',
                    new_query,
                    count=1,
                    flags=re.IGNORECASE
                )
            else:
                # Ajouter GROUP BY Ã  la fin
                new_query = f"{new_query.rstrip()}\nGROUP BY {dimension}"

            return new_query

    except Exception as e:
        print(f"[Proactive] Erreur gÃ©nÃ©ration requÃªte pour {dimension}: {e}")
        return None

    return None


def execute_drill_downs(
    client,
    original_query: str,
    dimensions: List[Tuple[str, str]],
    thread_ts: str,
    timeout: int
) -> Dict:
    """
    ExÃ©cute les requÃªtes de drill-down pour chaque dimension pertinente.
    Retourne un dict {dimension: {label: str, results: list}}
    """
    from bigquery_tools import _enforce_limit

    results = {}
    max_drill_downs = int(os.getenv("MAX_DRILL_DOWNS", "3"))

    # ğŸ†• VALIDATION : VÃ©rifier que les dimensions existent vraiment dans la table
    print(f"[Proactive] Validation des {len(dimensions)} dimensions souhaitÃ©es...")
    validated_dimensions = get_validated_dimensions(client, original_query, dimensions)

    if not validated_dimensions:
        print("[Proactive] Aucune dimension validÃ©e â€” skip drill-downs")
        return {}

    print(f"[Proactive] {len(validated_dimensions)} dimension(s) validÃ©e(s) sur {len(dimensions)}")

    for dimension, label in validated_dimensions[:max_drill_downs]:
        try:
            drill_query = generate_drill_down_query(original_query, dimension)

            if not drill_query:
                print(f"[Proactive] Impossible de gÃ©nÃ©rer requÃªte pour {dimension}")
                continue

            # ExÃ©cuter la requÃªte
            enforced_query = _enforce_limit(drill_query)

            print(f"[Proactive] ExÃ©cution drill-down sur {dimension}...")
            job = client.query(enforced_query)
            rows = list(job.result(timeout=timeout))

            if rows and len(rows) > 0:
                # Convertir en liste de dicts (max 10 lignes par dimension)
                rows_data = [dict(row) for row in rows[:10]]

                results[dimension] = {
                    "label": label,
                    "results": rows_data
                }

                print(f"[Proactive] âœ“ Drill-down {dimension}: {len(rows_data)} rÃ©sultats")
            else:
                print(f"[Proactive] âœ— Drill-down {dimension}: aucun rÃ©sultat")

        except Exception as e:
            print(f"[Proactive] âœ— Erreur drill-down {dimension}: {str(e)[:100]}")
            continue

    return results


def format_proactive_analysis(main_result: list, drill_down_results: Dict, context_type: str) -> str:
    """
    Formate les rÃ©sultats de l'analyse proactive multi-dimensionnelle.
    PrÃ©sente les drill-downs de maniÃ¨re claire et actionnable.
    """
    if not drill_down_results:
        return None

    output_lines = []
    output_lines.append("\n\n" + "="*60)
    output_lines.append("ğŸ” **ANALYSE PROACTIVE MULTI-DIMENSIONNELLE**")
    output_lines.append(f"_Franck a automatiquement explorÃ© {len(drill_down_results)} dimensions pertinentes pour le contexte '{context_type}' :_\n")

    for dimension, data in drill_down_results.items():
        label = data["label"]
        results = data["results"]

        if not results:
            continue

        output_lines.append(f"### ğŸ“Š Breakdown par **{label}**")

        # DÃ©tecter les colonnes de mÃ©trique (numÃ©riques)
        first_row = results[0]
        metric_cols = [k for k, v in first_row.items()
                      if isinstance(v, (int, float)) and k != dimension]

        if not metric_cols:
            output_lines.append("  _(Aucune mÃ©trique numÃ©rique trouvÃ©e)_\n")
            continue

        # Trier les rÃ©sultats par la premiÃ¨re mÃ©trique (desc)
        results_sorted = sorted(
            results,
            key=lambda x: x.get(metric_cols[0], 0) if x.get(metric_cols[0]) is not None else 0,
            reverse=True
        )

        # Calculer le total pour les pourcentages
        total_value = sum(row.get(metric_cols[0], 0) or 0 for row in results_sorted)

        # Formater chaque ligne (Top 5)
        for i, row in enumerate(results_sorted[:5], 1):
            dim_value = row.get(dimension, "N/A")

            # Formater les mÃ©triques
            metrics_parts = []
            for col in metric_cols:
                value = row.get(col, 0) or 0

                if isinstance(value, float):
                    metrics_parts.append(f"{col}={value:,.2f}")
                else:
                    metrics_parts.append(f"{col}={value:,}")

                # Ajouter le pourcentage du total pour la premiÃ¨re mÃ©trique
                if col == metric_cols[0] and total_value > 0:
                    pct = (value / total_value * 100) if value else 0
                    metrics_parts.append(f"({pct:.1f}%)")
                    break  # On ne montre le % que pour la premiÃ¨re mÃ©trique

            metrics_formatted = " | ".join(metrics_parts)

            # Emoji pour le top 3
            emoji = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else "  "

            output_lines.append(f"  {emoji} **{dim_value}** : {metrics_formatted}")

        if len(results_sorted) > 5:
            output_lines.append(f"  _... et {len(results_sorted) - 5} autres valeurs_")

        output_lines.append("")

    output_lines.append("="*60)

    return "\n".join(output_lines)
